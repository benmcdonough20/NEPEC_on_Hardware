{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GST data acquisition\n",
    "\n",
    "This notebook contains the code run on Rigetti hardware to carry out the pyGSTi gate set tomography. I ran into several issues that resulted in me wasting most of my compute time for this month, but in the end I was able to get some usable data. \n",
    "\n",
    "I first tried to use the `smq1Q_XZ` model pack to reflect the fact that the Rigetti hardware natively supports $RX(\\theta)$ and $RZ(\\theta)$ for universal Clifford implementation. The compile time was the bottleneck on the simulator, and so I used a `ThreadPool` to run the compilation in parallel. I added an active reset to my circuits to speed up the compute time once the real backend was used.\n",
    "\n",
    "I previously thought that compilation had to occurr during compute time to compile on a backend, like in Qiskit. It turns out that compilation for a real backend can be done locally to further take advantage of the compute time. I had verified my `settings.toml` and `secrets.toml` credential files on my QCS client. However the `run` method just idled and never executed. I quickly moved my code to the prepared Jupyterlabs notebook on the Rigetti server, and this time it ran quickly and I was able to extract data for the `smq1Q_XZ` modelpack. \n",
    "\n",
    "I wanted to have a full gate set to eventually use for PEC, so I moved to the `smq1Q_XYZI` modelpack. Even though the hardware does not implement Y natively, I thought I could still use the data for XZI for the tomography. However the umber of circuits doubled from ~780 to ~1500. This crashed the parallel pool and caused the kernel to restart. I tried playing with the number of workers, and separating the circuits into batches with garbage collection in between, but this didn't fix the problem. I switched to simple iteration, but the program still crashed. In an effort to save RAM, I used pickle to precompile and save the executables as binaries, and then unpickle them at runtime.\n",
    "\n",
    "Compilation remains the bottleneck, with 45mins-1hr of compilation for 15 minutes of computation on the QPU. I have since realized that I can carry out the compilation on my local machine and then upload the pickled binaries to the Jupyterlabs notebook, which will hopefully speed up the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyGSTi tools\n",
    "import pygsti\n",
    "\n",
    "#pre-built gateset to use\n",
    "from pygsti.modelpacks import smq1Q_XYZI\n",
    "from pygsti.data.dataset import DataSet\n",
    "from pygsti.io.writers import write_dataset\n",
    "\n",
    "#Rigetti tools\n",
    "from pyquil import get_qc, Program\n",
    "from pyquil.api import QCSClientConfiguration, local_forest_runtime\n",
    "from pyquil.gates import RESET\n",
    "\n",
    "configuration = QCSClientConfiguration.load()\n",
    "\n",
    "#python helper libraries\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create experiment design\n",
    "This is borrowed from the [tutorial notebook](https://github.com/pyGSTio/pyGSTi/blob/bfedc1de4d604f14b0f958615776fb80ddb59e33/jupyter_notebooks/Tutorials/algorithms/GST-Overview.ipynb) on GST with pyGSTi. The fiducials are a set of operators $\\{F_i\\}$ such that $F_i|\\rho \\rangle \\rangle$ and $\\langle \\langle E | F_i$ form an 'informationally complete' set. The germs are a set of strings generated from the target gates with the desired lengths, specified by `maxLengths`. This set is suppoed to be 'amplificationally complete', making it as sensitive to every kind of error as possible. The tutorials are not very clear on how to make these, and they are hard-coded into the model packs. I tried to re-impliment the `smq1Q-XYI` model pack and simply change every `Gypi2` to `Gzpi2` naively, but this did not affect the result. I would like to learn how to create custom gate sets, fiducials, and germs, but for now I am using the pre-build modelpacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = smq1Q_XYZI.target_model()      # a Model object\n",
    "prep_fiducials = smq1Q_XYZI.prep_fiducials()  # preparation circuits\n",
    "meas_fiducials = smq1Q_XYZI.meas_fiducials()  # measurement circuits\n",
    "germs = smq1Q_XYZI.germs()                    # circuits repeated to amplify noise\n",
    "maxLengths = [1,2,4,8,16,32]\n",
    "exp_design = pygsti.protocols.StandardGSTDesign(target_model, \n",
    "                                                prep_fiducials, \n",
    "                                                meas_fiducials,\n",
    "                                                germs, maxLengths) #stores data structure of experiment\n",
    "\n",
    "exp_design.all_circuits_needing_data\n",
    "circuits = list(exp_design.all_circuits_needing_data) #Get list of circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write empty protocol data, to be filled with experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pygsti.io.write_empty_protocol_data('experiment_data/rigetti_XYZI_data', exp_design, clobber_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next cell loads the qpu. The `execution_timeout` and `compiler_timeout` were necessary to handle queueing in the parallel pool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get QPU. Replace with simulator if no reservation\n",
    "qpu = get_qc(\"Aspen-11\", \n",
    "             client_configuration=configuration, \n",
    "             execution_timeout = 100000, \n",
    "             compiler_timeout = 100000) #I thought 27 hours seemed like a reasonable timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `circ_fname` function is a terrible last-minute fix designed like a bijective hash between circuits and filenames within the system filename character limit. These files hold the pickled binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circ_fname(circ):\n",
    "    return \"circuit_binaries/%s .circ\"%str(circ).replace(\"-\",\"\").replace('\\n','').replace('|','').replace(' ','').replace('G','')[8:]\n",
    "\n",
    "shots = 1000 #number of shots for each circuit. This was the default value\n",
    "num_circs = len(circuits)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for (i,circ) in enumerate(circuits):\n",
    "    #convert pyGSTi circuit to quil program. add active reset to speed up execution\n",
    "    prog = Program(circ.convert_to_quil()).wrap_in_numshots_loop(shots)\n",
    "    executable = qpu.compile(prog) #compile for target QPU\n",
    "    \n",
    "    with open(circ_fname(circ), \"wb\") as f:\n",
    "        pickle.dump(executable, f) #dump binary for later use\n",
    "    \n",
    "    #A watched pot will still boil eventually\n",
    "    print(\"finished \",i, \" Done in \",(time.time()-start_time)/(i+1)*(num_circs-i), end='\\r')\n",
    "    \n",
    "print(time.time() - start_time) #Around 1hr."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The compilation ended up being the bottleneck by far. The QPU runs jobs extremely quickly. It better at $40 per minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "#Run the program in the file identified by 'circ' and return results\n",
    "def run(circ):\n",
    "    \n",
    "    with open(circ_fname(circ), \"rb\") as f: #hash circuit back to executable file\n",
    "        executable = pickle.load(f)\n",
    "        \n",
    "    result = qpu.run(executable).readout_data.get(\"ro\")\n",
    "    zeros = len([i for i in result if i== [0]]) #count the number of zeros\n",
    "    return zeros\n",
    "\n",
    "#I still found issues with the parallel pool for execution,\n",
    "#so I made this part iterative too\n",
    "for (i,circ) in enumerate(circuits):\n",
    "    print(\"Running\", i,end='\\r' ) #For sanity\n",
    "    results.append(run(circ)) #add number of zeros from run to result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the results in a data set and write to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset object acts like a 2-d dictionary, with keys as circuit names,\n",
    "#followed by result outcome\n",
    "data = DataSet()\n",
    "\n",
    "for (circ, result) in zip(circuits, results):\n",
    "    data.add_count_dict(circ, {'0':result, '1':shots-result})\n",
    "    \n",
    "#write dataset for later usage\n",
    "write_dataset(\"experiment_data/rigetti_XYZI_data/data/dataset.txt\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On jupyterlabs the data folder needs to be compressed to download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "tar -cvf second_experiment_run_data.tar experiment_data circuit_binaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
